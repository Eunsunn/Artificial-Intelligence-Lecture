{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_SVM_201711719_심은선.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeOLHohoiJQT"
      },
      "source": [
        "# 인공지능 HW5\n",
        "### : SVM을 이용한 스팸 메일 분류\n",
        "- 학번: 201711719\n",
        "- 학과: 응용통계학과\n",
        "- 이름: 심은선\n",
        "- 제출날짜: 2020.10.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN-4_JR4iJQT"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FFXAi5GiLqo",
        "outputId": "14a2dd41-6e0f-47e3-9bee-a9db49329d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7s7Ll7xiJQU"
      },
      "source": [
        "import typing\n",
        "from typing import List"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn_Fr8IXiJQW"
      },
      "source": [
        "# 데이터 읽는 함수 정의\n",
        "def load_data(file_path : str):\n",
        "    # 파일 읽기\n",
        "    with open(file_path,'r',encoding='utf8') as inFile:\n",
        "        lines = inFile.readlines()\n",
        "\n",
        "    x_data, y_data = [], []\n",
        "    for line in lines:\n",
        "        pieces = line.strip().split('\\t')\n",
        "        label, sentence = pieces[0], pieces[1]\n",
        "        x_data.append(sentence)\n",
        "        y_data.append(label)\n",
        "        \n",
        "    return x_data, y_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWLWAtC9iJQY",
        "outputId": "32561e02-4e65-4b27-97a2-51ac29452534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "file_path = \"/gdrive/My Drive/인공지능(4-2)/wk5.SVM/SMSSpamCollection\"\n",
        "X, y = load_data(file_path)\n",
        "\n",
        "print(\"x_data의 개수 : \" + str(len(X)))\n",
        "print(\"y_data의 개수 : \" + str(len(y)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data의 개수 : 1500\n",
            "y_data의 개수 : 1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6sr9ZJSiJQZ"
      },
      "source": [
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXaanqsGiJQa"
      },
      "source": [
        "### 1) Tokenizer로 문장 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h256o7yYiJQa"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5s4XkapiJQc",
        "outputId": "2cdd17de-aad2-4b00-b70d-8528b669f425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리\n",
        "label2index = {'spam':0, 'ham':1}\n",
        "index2label = {0:\"spam\", 1:\"ham\"}\n",
        "\n",
        "# indexing 한 데이터를 넣을 리스트 선언\n",
        "indexing_X, indexing_y = [], []\n",
        "\n",
        "for label in y:\n",
        "  indexing_y.append(label2index[label])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=300) #상위 300개 단어로 vocabulary 구성\n",
        "\n",
        "# x_data를 사용하여 딕셔너리 생성 (vocabulary)\n",
        "tokenizer.fit_on_texts(X)                \n",
        "\n",
        "# x_data에 있는 각 문장들을 one-hot 벡터의 합으로 치환하고 그 결과값을 indexing_x_data에 저장\n",
        "indexing_X = tokenizer.texts_to_matrix(X, mode='count').tolist()\n",
        "\n",
        "print(\"x_data indexing 전 : \" + str(X[0]))\n",
        "print(\"x_data indexing 후 : \" + str(indexing_X[0]))\n",
        "print(\"y_data indexing 전 : \" + str(y[0]))\n",
        "print(\"y_data indexing 후 : \" + str(indexing_y[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_data indexing 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "x_data indexing 후 : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "y_data indexing 전 : ham\n",
            "y_data indexing 후 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDx7alG3iJQe"
      },
      "source": [
        "### 2) 카이제곱으로 문장 embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFyy0y1svW_H"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8g9Hyu8iJQe"
      },
      "source": [
        "# label로 문서 분리\n",
        "def split_doc(X:List, y:List, label:str):\n",
        "  ret = [x for idx, x in enumerate(X) if y[idx]==label]\n",
        "  return ret\n",
        "\n",
        "\n",
        "# 문장마다 단어 set을 얻는다.\n",
        "def word_in_a_doc(doc:str):\n",
        "  return set(doc.split())\n",
        "\n",
        "\n",
        "# 모든 문장의 단어 set 얻기\n",
        "def all_word(word_sets: List):\n",
        "  total_word = set()\n",
        "  for word_set in word_sets:\n",
        "    total_word  = total_word.union(word_set)\n",
        "  return total_word\n",
        "\n",
        "\n",
        "# 모든 단어를 반복하면서 카이제곱 구하기\n",
        "def word_chisquare(sets_1: List, sets_2: List, total_word: set):\n",
        "  chi = dict()\n",
        "  p_len = len(sets_1)\n",
        "  n_len = len(sets_2)\n",
        "  for word in total_word:\n",
        "    A = sum([ word in s for s in sets_1 ])\n",
        "    B = sum([ word in s for s in sets_2 ])\n",
        "    C = p_len - A\n",
        "    D = n_len - B\n",
        "    chi_val = (A+B+C+D) * ((A*D-C*B)**2)\n",
        "    chi_val /= ((A+C)*(B+D)*(A+B)*(C+D))\n",
        "    chi[word] = chi_val\n",
        "  return chi\n",
        "\n",
        "\n",
        "# 상위 N개의 카이제곱 단어 구하기\n",
        "def top_N_chiword(chi: dict, N: int):\n",
        "  chi_sort = sorted(chi.items(), key=lambda x: x[1], reverse=True)\n",
        "  N = min(N, len(chi))\n",
        "  words, val = zip(*chi_sort[:N])\n",
        "  return words, val\n",
        "\n",
        "\n",
        "# vocabulary 기반 voca-idx 딕셔너리 생성\n",
        "def word_to_idx(words):\n",
        "    voca_dict = {}\n",
        "    for idx, voca in enumerate(words):\n",
        "        voca_dict[voca] = idx\n",
        "    return voca_dict\n",
        "\n",
        "\n",
        "# vocabulary로 문장 임베딩 : 포함여부 (0, 1)로 임베딩\n",
        "def embed_docs(docs: List, vocab: set):\n",
        "    word2idx = word_to_idx(vocab)\n",
        "    vec_dim = len(vocab)\n",
        "    N = len(docs) # 문서 개수\n",
        "    \n",
        "    docs_embed = np.zeros((N, vec_dim)) # 문서임베딩 벡터의 모임 -> 행렬\n",
        "    for i, doc in enumerate(docs):\n",
        "        words = doc.split()\n",
        "        for word in words:\n",
        "            if word in vocab: # 단어가 vocabulary에 있을 때만\n",
        "                idx = word2idx[word]\n",
        "                docs_embed[i][idx] = 1 # 포함 여부만\n",
        "    return docs_embed"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNiIFv0rok0U"
      },
      "source": [
        "spam_X = split_doc(X, y, \"spam\")\n",
        "ham_X = split_doc(X, y, \"ham\")\n",
        "\n",
        "spam_X_sets = [word_in_a_doc(doc) for doc in spam_X]\n",
        "ham_X_sets = [word_in_a_doc(doc) for doc in ham_X]\n",
        "total_word_set = all_word(spam_X_sets+ham_X_sets)\n",
        "\n",
        "chi_square = word_chisquare(spam_X_sets, ham_X_sets, total_word_set)\n",
        "top_N_word, _ = top_N_chiword(chi_square, 200)\n",
        "\n",
        "chi_embedding = embed_docs(X, top_N_word)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "horrRivwvciU",
        "outputId": "0faa2442-1edb-42ee-a226-fa13d4c6b8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chi_embedding.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIJDTK6diJQg"
      },
      "source": [
        "### 3) Tokenizer, 카이제곱 embedding 합치기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Misbt-TiJQg"
      },
      "source": [
        "embedding_X = np.concatenate((indexing_X, chi_embedding), axis=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWGAn5lZiJQi",
        "outputId": "82efe67f-a6fd-44c9-b6c7-ef76d1444467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ev5lPdiJQk"
      },
      "source": [
        "# 3. Training & prediction: SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBdwBCNPiJQk"
      },
      "source": [
        "### 1) Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1x2JGYQwbGI"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Egpjk9SiJQl"
      },
      "source": [
        "# train:test = 9:1 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(embedding_X, indexing_y, test_size=0.1, random_state=2020)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOkYggA4w0V4",
        "outputId": "6824f16b-3919-4d43-b510-0cb205391543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(X_train, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb-IKGlciJQn"
      },
      "source": [
        "### 2) prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2S1aEqYxHxg"
      },
      "source": [
        "predict = svm.predict(X_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAl2z_4CiJQp"
      },
      "source": [
        "### 3) 모델 성능 평가: Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYq1XYT1iJQp"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uV4YWpbiJQr",
        "outputId": "10affe2b-ff7a-4de5-eba5-bf0a068468fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy: %.2f' % accuracy_score(y_test, predict))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noQL_O1Sxvx4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}