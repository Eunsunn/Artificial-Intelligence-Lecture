{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공지능 과제4\n",
    "### : Naive Bayes 기반 문장 감성분류\n",
    "- 담당 교수님: 김학수 교수님\n",
    "- 학번: 201711719\n",
    "- 학과: 응용통계학과\n",
    "- 이름: 심은선\n",
    "- 제출날짜: 2020.09.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 필요 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) TF-IDF vocab 추출 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서별 TF-IDF 구하기\n",
    "def get_TFIDF(docs: List):\n",
    "\n",
    "    # 문서별 TF 구하기 (문서별로 단어 등장 횟수 count, 공백 기준 분리)\n",
    "    def get_TF(docs: List):\n",
    "        TF = [] \n",
    "        for idx, text in enumerate(docs):\n",
    "            words = text.split()\n",
    "            word_count = collections.Counter(words)\n",
    "            TF.append(word_count)\n",
    "        return TF\n",
    "\n",
    "    # DF 구하기\n",
    "    def get_DF(docs: List):\n",
    "        word_ls = []\n",
    "        for doc in docs:\n",
    "            word_ls += list(set(doc.split()))\n",
    "        df = collections.Counter(word_ls) #문서 전체의 df\n",
    "        return df\n",
    "    \n",
    "    tf = get_TF(docs)\n",
    "    df = get_DF(docs)\n",
    "    N = len(docs)\n",
    "    \n",
    "    docs_tf_idf = []\n",
    "    for word_count in tf: #문서마다 반복\n",
    "        doc_tf_idf = dict()\n",
    "        for word, count in word_count.items(): # 문서의 단어별로 tf-idf를 구한다\n",
    "            word_tf_idf = count * np.log(N / df[word])\n",
    "            doc_tf_idf[word] = word_tf_idf\n",
    "        docs_tf_idf.append(doc_tf_idf)\n",
    "    return docs_tf_idf\n",
    "\n",
    "\n",
    "# tf_idf가 높은 상위 k개의 단어 리스트 얻기\n",
    "def get_freq_word(tf_idf: dict, k: int):\n",
    "    sort_docs_tf_idf = list(sorted(tf_idf.items(), key=lambda x: x[1], reverse=True))[:k]\n",
    "    vocab = list(zip(*sort_docs_tf_idf))[0]\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "# vocabulary 기반 voca-idx 딕셔너리 생성\n",
    "def word_to_idx(words):\n",
    "    voca_dict = {}\n",
    "    for idx, voca in enumerate(words):\n",
    "        voca_dict[voca] = idx\n",
    "    \n",
    "    return voca_dict\n",
    "\n",
    "\n",
    "# 문장을 voca count로 임베딩(tf-idf 상위 k개의 vocabulary 기반 Count Vectorizer임베딩)\n",
    "def embed_docs(docs: List, vocab: set):\n",
    "    word2idx = word_to_idx(vocab)\n",
    "    vec_dim = len(vocab)\n",
    "    N = len(docs) # 문서 개수\n",
    "    \n",
    "    docs_embed = np.zeros((N, vec_dim)) # 문서임베딩 벡터의 모임 -> 행렬\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if word in vocab: # 단어가 vocabulary에 있을 때만\n",
    "                idx = word2idx[word]\n",
    "                docs_embed[i][idx] += 1\n",
    "    \n",
    "    return docs_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 파일 읽는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# (X \\t y)로 구성된 dataset을 읽어서 X, y로 각각 저장한다.\n",
    "def load_data(data_dir, file_path):\n",
    "    file = open(os.path.join(data_dir, file_path), encoding='UTF-8') #윈도우에서 cp949 파일 열기위해 encoding지정\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for line in tqdm(file.readlines()):\n",
    "        text, label = line.strip().split('\\t')\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) TF-IDF vocab 구성\n",
    "- 긍정 문서 상위 100개, 부정 문서 상위 100개 words 사용\n",
    "- set(positive words + negative words)으로 구성된 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 2686/2686 [00:00<00:00, 1328682.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 300954.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_data(os.path.join(os.getcwd(), 'data'), 'sentiment_train.txt')\n",
    "test_X, test_y = load_data(os.path.join(os.getcwd(), 'data'), 'sentiment_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positive/negative document respectively\n",
    "pos_idx = [ i for i, y in enumerate(train_y) if y==\"<P>\"]\n",
    "neg_idx = [ i for i, y in enumerate(train_y) if y==\"<N>\"]\n",
    "train_pos_df = np.array(train_X)[pos_idx]\n",
    "train_neg_df = np.array(train_X)[neg_idx]\n",
    "\n",
    "train_pos_X, train_neg_X = \"\", \"\"\n",
    "for pos, neg in zip(train_pos_df, train_neg_df):\n",
    "    train_pos_X += \" \" + pos\n",
    "    train_neg_X += \" \" + neg\n",
    "\n",
    "# Get tf-idf of positive, negative document\n",
    "tf_idf = get_TFIDF([train_pos_X, train_neg_X])\n",
    "\n",
    "# Get top 100 frequent words respec.\n",
    "top_words = 100\n",
    "pos_words = get_freq_word(tf_idf[0] ,top_words)\n",
    "neg_words = get_freq_word(tf_idf[1] ,top_words)\n",
    "\n",
    "# Get total vocab (remove redundant words)\n",
    "vocab = set(pos_words + neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 voca 개수: 200 개\n"
     ]
    }
   ],
   "source": [
    "print(\"전체 voca 개수: {} 개\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 문서 Embedding\n",
    "- 추출한 TF-IDF 기반 문서 임베딩\n",
    "- CountVectorizer 방식\n",
    "- vocabulary에 없는 단어 제거, 빈 문자열 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary에 없는 문자 제거\n",
    "def postprocessing(docs: List, vocab: set) -> List:\n",
    "    ret = []\n",
    "    for doc in docs:\n",
    "        string = \"\"\n",
    "        for word in doc.split():\n",
    "            if word in vocab: string += \" \" + word\n",
    "        string = string.strip()\n",
    "        ret.append(string)\n",
    "    return ret\n",
    "\n",
    "# 빈 문자열 제거\n",
    "def remove_null(docs_X: List, docs_y: List):\n",
    "    ret_X = []\n",
    "    ret_y = []\n",
    "    for idx in range(len(docs_X)):\n",
    "        if docs_X[idx]: # 빈 문자열이 아닌 경우만 사용\n",
    "            ret_X.append(docs_X[idx])\n",
    "            ret_y.append(docs_y[idx])\n",
    "    return ret_X, ret_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = postprocessing(train_X, vocab)\n",
    "test_X = postprocessing(test_X, vocab)\n",
    "\n",
    "train_X, train_y = remove_null(train_X, train_y)\n",
    "test_X, test_y = remove_null(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab을 기반으로 train_X를 임베딩한다.\n",
    "train_X_embedding = embed_docs(train_X, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes 기반 문장 감성분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_X_embedding, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) prediction on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test는 transform만! (train voca로 transform 해야함)\n",
    "test_X_embedding = embed_docs(test_X, vocab)\n",
    "predictions = nb_clf.predict(test_X_embedding).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) 성능평가\n",
    "- accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: %.5f' % accuracy_score(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
